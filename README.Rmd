---
title: "README"
output: github_document
---

## Background

This project is for implementing the statistical and machine learning techniques from reading the 2nd edition of Introduction to Statistical Learning. I want to grow my understanding and usage of prediction/inference techniques. I also want to use the language `R` since there are reliable and versatile APIs from the {tidymodels} collection of packages. Also, the visualization of the outputs will be a lot better and easier if I stay within the `R` environment and use the {ggplot2} package. 

I will eventually use {targets} to orchestrate the modeling pipeline. There are many steps involved when using these techniques and as the number of steps grow it will be hard to keep track of the analysis components. To enforce reproducibility and keep track of the analysis pipeline, I will eventually use {targets} to orchestrate the pipeline as it grows. There are a few main components that will be part of the pipeline:

- Dataset selection
- Exploratory data analysis
- Feature Selection & Engineering
- Model Training
- Model Evaluation
- Model Testing

First, I will be selecting different datasets for both applying classification and regression techniques. In both cases, I will choose a response variable but the predictors will be automatically selected from the other variables in the dataset. Next, I will produce different plots and summaries to have an understanding of the relationship between the response and potential predictors. Next, I will perform feature selection using the LASSO technique to identify predictors related to the response. I will utilize the bootstrap to identify the predictors with a non-zero relationship to the response. Using these predictors, I will then train a pre-defined set of models. I will utilize cross validation and different hyperparameter tuning techniques to evaluate the best performing model that maximizes the tradeoff between sensitivity and specificity (the roc_auc metric). Then using the maximal model, I will retrain te model on the entire training dataset to derive the predictor importances to the response. Lastly I will predict the held out test data and compare the different model performances. 

```{r load_libraries}
library(tidymodels)
library(discrim)
library(MASS)
```

```{r run_example_workflow}

all_data <- modeldata::ad_data |> mutate(male = factor(male,levels=c(0,1)))
rsplits <- rsample::initial_split(all_data)
train_data <- training(rsplits)
val_data <- testing(rsplits)

outcome_vars <- "Class"
predictor_vars <- setdiff(colnames(train_data),outcome_vars)
all_vars <- c(outcome_vars,predictor_vars)
var_roles <- c(
    rep("outcome",length(outcome_vars)),
    rep("predictor",length(predictor_vars))
)

base_rec <- 
    recipe(train_data,vars=all_vars,roles=var_roles) |> 
    step_dummy(all_nominal_predictors())
norm_rec <- 
    recipe(train_data,vars=all_vars,roles=var_roles) |> 
    step_normalize(all_numeric_predictors()) |> 
    step_dummy(all_nominal_predictors())

specs <- 
  list(
    "Logit" = parsnip::logistic_reg(penalty = tune(),mixture = tune()) |> parsnip::set_engine("glmnet"),
    "LDA" = parsnip::discrim_linear(regularization_method = "diagonal",penalty = 1) |> set_engine("MASS"),
    "SVM" = svm_linear(cost = tune(), margin = tune()) %>% set_engine("kernlab") %>% set_mode("classification")
  )

folds <- vfold_cv(train_data,v = 10,repeats = 10)

set.seed(8241)
doParallel::registerDoParallel()
wf_set <- workflow_set(
    preproc = list(
        base = base_rec,
        normalized = norm_rec
        ),
    models = specs
)
res <- 
    workflow_map(
    wf_set,
    fn = 'tune_bayes',
    resamples = folds,
    verbose = TRUE,
    metrics = metric_set(accuracy, sensitivity, specificity)
)

```


```{r plot_example_workflow}
autoplot(res)
```
